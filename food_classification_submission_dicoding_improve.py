# -*- coding: utf-8 -*-
"""food_classification_submission_dicoding_improve.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fol7ZyRaYpPMwlJkMce5cS_2y-8ZdR_n

### Nama Lengkap : MOHAMAD ARIF SOFYAN
### User name Dicoding : mohamad_arif_sofyan
### Dataset link : https://www.kaggle.com/datasets/trolukovich/food11-image-dataset

# Import Library
"""

!pip install -q kaggle
!pip install split-folders

from google.colab import files
import os
import splitfolders
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import tensorflow as tf
import seaborn as sns
import numpy as np
import shutil
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Flatten
from tensorflow.keras.layers import Input
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam, SGD, Adamax
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard
from tensorflow.keras.applications import MobileNetV2, InceptionResNetV2
from tensorflow.keras.regularizers import l2
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.utils import plot_model
from sklearn.metrics import confusion_matrix, classification_report

"""# Pengaturan Kaggle API untuk Mengunduh Dataset"""

files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

"""# Mengunduh dan Mengekstrak Dataset"""

!kaggle datasets download -d trolukovich/food11-image-dataset
!unzip -q food11-image-dataset.zip -d dataset

"""# Mengeksplorasi Dataset"""

train = {}
val = {}
eval = {}

path = "/content/dataset"

path_training = os.path.join(path, "training")
for i in os.listdir(path_training):
    train[i] = os.listdir(os.path.join(path_training, i))

path_validasi = os.path.join(path, "validation")
for i in os.listdir(path_validasi):
    val[i] = os.listdir(os.path.join(path_validasi, i))

path_evaluation = os.path.join(path, "evaluation")
for i in os.listdir(path_evaluation):
    eval[i] = os.listdir(os.path.join(path_evaluation, i))

total_train_images = sum(len(images) for images in train.values())
print(f"Total jumlah gambar dalam data training: {total_train_images}")

total_val_images = sum(len(images) for images in val.values())
print(f"Total jumlah gambar dalam data validation: {total_val_images}")

total_eval_images = sum(len(images) for images in eval.values())
print(f"Total jumlah gambar dalam data evaluation: {total_eval_images}")

"""# Dataset Splitting"""

input_folder = "/content/dataset/training"
output_folder = "food_dataset"

splitfolders.ratio(input_folder, output=output_folder, seed=42, ratio=(0.8, 0.2), group_prefix=None)

path_train = "/content/food_dataset/train"
path_test = "/content/food_dataset/val"

def explore_data(path):
    data = {}
    for category in os.listdir(path):
        data[category] = os.listdir(os.path.join(path, category))
    return data

train_data = explore_data(path_train)
test_data = explore_data(path_test)

def print_image_count(data, data_type):
    total_images = sum(len(images) for images in data.values())
    print(f"Total jumlah gambar dalam data {data_type}: {total_images}")
    for category in data:
        print(f"Kategori {category}: {len(data[category])} gambar")

print_image_count(train_data, "latih")
print("\n")
print_image_count(test_data, "uji")

"""# Visualisasi Sampel Data"""

def display_sample_images(data, path, num_images=5):
    for category, images in data.items():
        image_paths = [os.path.join(path, category, img) for img in images[:num_images]]
        plt.figure(figsize=(15, 5))
        for i, img_path in enumerate(image_paths):
            plt.subplot(1, num_images, i+1)
            img = mpimg.imread(img_path)
            plt.imshow(img)
            plt.title(f"{category}")
            plt.axis('off')
        plt.show()

display_sample_images(train_data, path_train)

"""# Preprocessing Data dan Data Augmentation"""

TARGET_SIZE = (331, 331)
BATCH_SIZE = 32
EPOCHS = 50

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

validation_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    path_train,
    target_size=TARGET_SIZE,
    batch_size=BATCH_SIZE,
    shuffle=True,
    seed=42,
    class_mode='categorical'
)

validation_generator = validation_datagen.flow_from_directory(
    path_test,
    target_size=TARGET_SIZE,
    batch_size=BATCH_SIZE,
    shuffle=False,
    class_mode='categorical'
)

"""# Membangun model sequential dengan Conv2D Maxpooling Layer dan Transfer Learning (InceptionResNetV2)"""

input_tensor = Input(shape=(331, 331, 3))
base_model = InceptionResNetV2(weights="imagenet", include_top=False, input_tensor=input_tensor)
base_model.trainable = True

for layer in base_model.layers:
    layer.trainable = True

model = Sequential([
    base_model,
    Conv2D(512, (3, 3), activation='relu', padding='same'),
    MaxPooling2D(2, 2),
    Conv2D(256, (3, 3), activation='relu', padding='same'),
    MaxPooling2D(2, 2),
    GlobalAveragePooling2D(),
    Dense(1024, activation='relu', kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    Dropout(0.5),
    Dense(512, activation='relu', kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    Dropout(0.4),
    Dense(11, activation='softmax')
])

optimizer = Adamax(learning_rate=1e-4)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()
plot_model(model, to_file='model_graph.png', show_shapes=True, show_layer_names=True)

class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_generator.classes),
    y=train_generator.classes
)
class_weights = dict(enumerate(class_weights))

"""# Callback"""

class MyCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if(logs.get('val_accuracy') > 0.93):
            print("\nMencapai 93% akurasi pada validation set, menghentikan pelatihan!")
            self.model.stop_training = True

model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, save_weights_only=False, monitor='val_accuracy', mode='max', verbose=1)
early_stopping = EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True, mode='max', verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=5, min_lr=1e-6, mode='max', verbose=1)
tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)
custom_callback = MyCallback()
callbacks_list = [model_checkpoint, early_stopping, reduce_lr, tensorboard, custom_callback]

"""# Melatih model"""

history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // BATCH_SIZE,
    epochs=EPOCHS,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // BATCH_SIZE,
    class_weight=class_weights,
    callbacks=callbacks_list
)

"""# Visualisasi Akurasi dan Loss"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'r', label='Training Loss')
plt.plot(epochs, val_loss, 'b', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.legend()

plt.show()

"""# Evaluasi"""

train_eval_result = model.evaluate(train_generator, verbose=1)

train_accuracy = train_eval_result[1]
train_loss = train_eval_result[0]

print(f'Training Accuracy: {train_accuracy * 100:.2f}%')
print(f'Training Loss: {train_loss:.4f}')

eval_result = model.evaluate(validation_generator, verbose=1)

accuracy = eval_result[1]
loss = eval_result[0]

print(f'Validation Accuracy: {accuracy * 100:.2f}%')
print(f'Validation Loss: {loss:.4f}')

y_pred = model.predict(validation_generator)
y_pred_classes = np.argmax(y_pred, axis=1)

y_true = validation_generator.classes

class_labels = list(validation_generator.class_indices.keys())

conf_matrix = confusion_matrix(y_true, y_pred_classes)

plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

class_report = classification_report(y_true, y_pred_classes, target_names=class_labels)
print('Classification Report:\n', class_report)

"""# Menyimpan Model ke dalam Format TF-Lite:"""

export_dir = 'saved_model_food/'
tf.saved_model.save(model, export_dir)

converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

tflite_model_file = 'food.tflite'
with open(tflite_model_file, 'wb') as f:
    f.write(tflite_model)

print(f'Model berhasil diubah menjadi {tflite_model_file}')

"""# Extract Folder Model ke zip"""

export_dir = 'saved_model_food/'
zip_path = '/content/saved_model_food.zip'
shutil.make_archive('/content/saved_model_food', 'zip', export_dir)

